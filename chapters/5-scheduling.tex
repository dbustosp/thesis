\chapter{Estrategias de planificación de queries}
\label{cap:planificacion}
Los motores de búsqueda verticales son sistemas dedicados a un solo propósito e ideado con el propósito de lidiar con cargas de trabajos dinámicas. Un ejemplo de un motor de búsqueda vertical es un motor de publicidad que ejecuta una consulta cada vez que un usuario abre un correo electrónico en por ejemplo, el servicio de \textit{Yahoo! mail}; de esta forma se muestra publicidad de acuerdo al contenido del correo electrónico. Eventualmente millones de usuarios concurrentes están conectados a sus correos electrónicos, por lo que la carga de trabajo esperada para el motor de búsqueda puede llegar a órdenes de las cien mil consultas por segundo \citep{Gil-Costa:2013}. Adicionalmente, el hecho que las actualizaciones en un motor de búsqueda vertical ocurran con mayor frecuencia que en uno de propósito general, hace que el diseño de los algoritmos para procesar las \textit{queries} sea diferente; también se debe permitir la actualización del índice invertido.

Por lo anteriormente mencionado, se hace imperioso tener un sistema diseñado que soporte altas cargas de trabajo, y las respuestas a consultas esten en una cota de tiempo aceptable para el usuario sin mermar la calidad de los resultados obtenidos. También es necesario que las estructuras de datos y los algoritmos implementados soporten la concurrencia entre las transacciones de lecturas y escrituras; dicho de otra forma, eventualmente el motor de búsqueda tendrá que dejar de procesar consultas para poder servir las transacciones de escritura que actualizan el índice invertido.

A continuación se muestra las diferentes estrategias de planificación de transacciones de lectura abordadas en el presente trabajo utilizando diferentes enfoques; se presentan tres enfoques diferentes: El primero consiste en crear bloques de consultas en donde previamente a cada una de ellas se le asigna el número de hebras que utilizará en su resolución, luego el bloque es procesado en paralelo por los diferentes hilos de ejecución asignados; El segundo enfoque corresponde a unidades de trabajos, en la que a cada \textit{query} se le asigna un número determinado de unidades de trabajo y los \textit{threads} compiten por ellas desde una cola; (3) El tercer enfoque y último es el más básico, cada hilo de ejecución se hace cargo de una consulta y lleva a cabo su procesamiento, en este enfoque la competencia entre los hilos de ejecución es por las consultas. 


\section{Estrategias por bloques}
\label{scheduling:bloques}
Un sistema de planificación de un motor de búsqueda trabaja en un contexto \textit{online}, esto significa que desconoce las transacciones que vendrán en el futuro y que cuando llega una nueva transacción de lectura, se debe tomar una decisión rápida acerca de qué hacer con ella. En el contexto del presente trabajo, para que el planificador tome una decisión se debe conocer el número de hebras con los que cada \textit{query} será resuelta y para lo cual se utiliza los predictores mostrados en \ref{cap:prediccion}; una vez que se predice el tiempo, se asigna el numero de hilos de ejecución con los que la consulta será resuelta para cumplir con la cota de tiempo impuesta.  

Bajo el contexto de un motor de búsqueda en el que se debe planificar transacciones de lecturas que eventualmente serán resueltas de forma paralela por diferentes hilos de ejecución, existe una estrategia teórica llamada FR que aborda este problema \citep{Ye:2007} y se adapta a nuestro escenario de un motor de búsqueda vertical; además se propone dos nuevas estrategias siguiendo el mismo enfoque de FR, pero estas enfocadas principalmente en mejorar la asignación de consultas a bloques, para así reducir el tiempo ocioso de las hebras. 

\subsection{Estrategia FR}
\label{scheduling:fr}
La estrategia de planificación FR asume que cada query que llega al motor de búsqueda posee el tiempo en que se domorará en ser procesada. 
Sehace una clsificación de las queries entre Big y Small con el objetivo de crear estructuras de datos denominadas Rooms y Walls, en donde Walls y Tooms estarán formadas por queries Bigs y Smalls respectivamente. Ambas estructuras tienen un número máximo de máquinas disponibles para procesar las queries. Una consulta es Big si el número de máquinas requeridas para procesarlas es m (siendo m el número de máquinas disponibles),, de lo contrario, la query es small. 
% Debe estar bloqueada porque eventualmente el ejecutador de queries estará sacando

% ----  Descripción del algoritmo ----
Como se puede ver en el Algoritmo \ref{alg:fr}, cuando una nueva query llega al sistema se analiza si esta es de tipo Big o Small; esto se hace en el método isBig(), que retorna verdadero si es que el número de máquinas requeridas para procesarla es igual al máximo de máquinas disponibles, de lo contrario retorna falso. Si la consulta es big, entonces se crea un nuevo Wall, se agrega la query al bloque y el bloque es planificado en la lista de planificación SchedulingList. Si se está en presencia de una transacción de lectura Small, entonces se busca algún bloque disponible para planificar la query, esto se hace desde el primer bloque abierto para recibir transacciones de lectura hasta el final de la schedulingList. Finalmente, si es eventualmente no se encuentra algún bloque disponible para planificar la query, entonces se crea un nuevo bloque Room, se asigna la consulta al bloque y se asigna el bloque a la lista de scheduling. Cabe destacar que se dice que un bloque está abierto cuando (isOpen) cuando aún le quedan máquinas disponibles o cuando el proceso de ejecución ya ha tomado las queries de este bloque para resolverlas.
%----- Fin descripción algoritmo ----

\begin{algorithm}[!th]
\caption{\em $schedulerFR::assignQuery(L, Q)$: Planificación de consulta estrategia FR}
\label{alg:fr}
\begin{algorithmic}[1]
\REQUIRE Una SchedulingList $L$ en donde se hará la planificación, QueryObject $Q$ a planificar
\ENSURE SchedulingList $L$ con la nueva query planificada

\IF {$isBig(query)$}
	\STATE $block = new Wall();$
	\STATE $block \rightarrow addQuery(query);$
	\STATE $L \rightarrow addBlock(block);$
\ELSE
	\STATE $asignada = false;$
	\FOR {$ i = L \rightarrow firstOpenBlockLocked()...L \rightarrow sizeLocked()$}
		\STATE $room\_block = L \rightarrow getBlockLocked(i);$
		
		\IF {$(room\_block \rightarrow isOpen()) AND 
				(room\_block \rightarrow freeThreads() >= query \rightarrow getThreads())$
			}
			\STATE $room\_block \rightarrow addQuery(query)$
			\STATE $asignada = true$
			\STATE $break;$
		\ENDIF
	\ENDFOR
	
	\IF {$!(asignada)$}
		\STATE $block = new Room();$
		\STATE $block \rightarrow addQuery(query);$
		\STATE $L \rightarrow addBlockLocked(block);$		
	\ENDIF
\ENDIF

\end{algorithmic}
\end{algorithm}


// Esquema de ejecución

// Algoritmo

// Ejemplo de cómo van quedando los bloques

Se intuye que habrá pérdida de eficiencia al final de cada bloque. 


\subsection{Estrategia Times}
\label{scheduling:times}
La idea de esta estrategia es separar las queries que tengan tiempos de procesamiento muy diferentes, es decir, se crearán bloques de queries que tienen poca diferencia de tiempo, de esta forma se quiere reducir el tiempo perdido entre un bloque y otro. Cada bloque b tendrá un tiempo t, que será el menor tiempo perteneciente a alguna de las consultas ya planificadas en el bloque. La métrica establecida para que una query sea planificada en un bloque, es que el tiempo del bloque tb no puedo doblar al tiempo de la consulta tq, y viceversa. Si esta condición falla, entonces significa que la query q que se está intentando planificar posee tiempos que se escapa a los rangos de tiempo del bloque b. A continuación se muestra el algoritmo para la presente estrategia

\begin{algorithm}[!th]
\caption{\em $schedulerTimes::assignQuery(L, Q)$: Planificación de consulta estrategias Times}
\label{alg:times}
\begin{algorithmic}[1]
\REQUIRE Una SchedulingList $L$ en donde se hará la planificación, QueryObject $Q$ a planificar
\ENSURE SchedulingList $L$ con la nueva query planificada
	\STATE $blocks_viewed = 0$
	
	\FOR {$ i = L \rightarrow firstOpenBlockLocked()...L \rightarrow sizeLocked()$}
		\STATE $block = lista\_bloques \rightarrow getBlockLocked(i);$
		
		\IF {$block \rightarrow isOpen() AND block \rightarrow freeThreads() \geq query \rightarrow getThreads()$}
			\STATE $queries = block \rightarrow getQueries();$
			\STATE $tiempo\_min = queries[0] \rightarrow getEstimatedTime();$
			
			\FOR {$ j = 1 ...queries->size()$}
				\IF {$queries[j] \rightarrow getEstimatedTime() < tiempo_min$}
					\STATE $tiempo\_min = queries[j] \rightarrow getEstimatedTime();$
				\ENDIF
			\ENDFOR
			
			\IF {$tiempo\_min < estimated\_time$}
				\STATE $diff = 2*tiempo_min - estimated_time;$
			\ELSE
				\STATE $diff = 2*estimated_time - tiempo_min;$
			\ENDIF
			
			\IF {$diff < 0$}
				\STATE $diff = 0.0-diff;$
				\IF {$diff < best_diff$}
					\STATE $best_diff = diff;$
					\STATE $best_block = i;$
				\ENDIF
			\ELSE
				\STATE $valido = true;$
			\ENDIF
			
			\IF {$ valido $}
				\STATE $block \rightarrow addQuery(query);$
				\STATE $asignada = true;$
				\STATE $break;$
			\ENDIF
			
			\STATE $bloques_revisados++;$
			
			\IF {$bloques_revisados \geq max_bloques_rev$}
				\STATE $break;$
			\ENDIF
			
		\ENDIF
		
	\ENDFOR
	
	\IF {$ ! asignada $}
		\IF {$ (bloques\_revisados \geq max\_bloques_rev) $}
			\STATE $block = lista\_bloques \rightarrow getBlockLocked(best\_block);$
			\STATE $block \rightarrow addQuery(query);$
		\ELSE
			\STATE $block = new QueryBlock();$
			\STATE $block \rightarrow addQuery(query);$
			\STATE $lista\_bloques \rightarrow addBlockLocked(block);$
			
		\ENDIF	
	\ENDIF

\end{algorithmic}
\end{algorithm}




\subsection{Estrategia TimesRanges}
\label{scheduling:fr}


\section{Estrategia unidades de trabajo}
\label{scheduling:unidadestrabajo}
Decir que aquí para planificar se necesita conocer las unidades de trabajos requeridas en vez de threads como en las estrategias por bloques.

Con respecto a los esquemas explicados hasta ahora, el esquema 1TQ tiene la ventaja que no solo requiere menos control, sino que también permite a los hilos de ejecución trabajar sin pausa mientras un \textit{batch} de consultas está siendo procesado. En esta sección se propone un esquema híbrido basado en unidades de procesamiento (\textit{Processing Units}) que aproveche las ventajas de ambos enfoques. (se requiere ver el tema de bloques).
En este nuevo esquema de planificación, las consultas pasan a través de una fase en la cual cada \textit{query} es evaluada y se determina un apropiado número de unidades de procesamiento (\textit{processing units}) para poder resolver dicha consulta. Este proceso es llevado a cabo de manera similar al proceso en donde se determina la cantidad de \textit{threads} apropiados para resolver una determina transacción de lectura. Este número de unidades de procesamiento es creado y asociado a cada consulta, finalmente se guarda en una cola de unidades de trabajo. Un conjunto de \textit{threads} consumidores extraen las unidades desde la cola y las procesa independientemente. Cuando un \textit{thread} finalice el procesamiento de la unidad de trabajo actual automáticamente leerá la siguiente unidad de trabajo desde la cola. 
Generalmente lo que se hace habitualmente es estimar el número de \textit{threads} con el que se resolverá la consulta, como se muestra en la Figura \ref{fig:unit_process} en este nuevo enfoque se intenta estimar el número de unidades de trabajo con el que se resolverá cada consulta. Además, se debe controlar el acceso concurrente de los hilos de ejecución a la cola de unidades de trabajo, de tal manera que solo un thread tenga acceso exclusivo a la estructura de datos. 

\begin{figure}[!th]
\centering
\includegraphics[scale=.75]{images/unit_process.eps}
\caption{Procesamiento de consultas utilizando unidades de trabajo}
\label{fig:unit_process}
\end{figure}

El procesamiento de cada hilo de ejecución es una versión de Wand con heap compartido (SH), adaptado de manera tal que cada unidad de trabajo es resuelta independientemente de si existen otras unidades siendo procesada al mismo tiempo o no. La única excepción es que la unidad que inicializa la consulta es siempre ejecutada antes del resto de las otras unidades de la misma consulta y la entrega de resultados se hace una vez que todas las unidades de trabajo de la \textit{query} han finalizado. Este enfoque híbrido permite reducir el tiempo perdido al final de cada \textit{batch} sin generar una importante pérdida de trabajo mientras las \textit{queries} del \textit{batch} están siendo procesadas.



\section{Estrategia \textit{1TQ}}
\label{scheduling:baseline}
Un simple camino para construir un sistema que responda a múltiples consultas simultáneamente usando múltiple hilos de ejecución, es usando estos hilos de manera independiente. Para hacer esto se debe mantener un conjunto de \textit{threads} consumidores que trabajarán en paralelo y se encargarán de resolver las \textit{queries} secuencialmente (una a una) desde una misma cola, esto es lo que en este trabajo se denomina estrategia de Un Thread Por Query (1TQ). En la Figura \ref{fig:1TQ} se puede apreciar el esquema de ejecución en donde cada uno de los procesos genera una petición de alguna consulta en la cola, si quedan \textit{queries} por procesar entonces se le asigna al proceso una consulta que tendrá que resolver de manera secuencial. Se debe tener en cuenta que cada vez que un proceso genera una solicitud de \textit{query}, se bloquea la estructura de datos que contiene las consultas a procesar y luego se procesa la solicitud, de esta forma se asegura un acceso seguro por parte de los distintos \textit{threads}. 

\begin{figure}[H]
\centering
\includegraphics[scale=.75]{images/1TQ.eps}
\caption{Ejemplo de procesamiento estrategia 1TQ}
\label{fig:1TQ}
\end{figure}

Este esquema tiene la ventaja que es simple y fácil de implementar y controlar. Sin embargo, existen sistemas de recuperación de la información como los motores de búsqueda verticales que cuando están ejecutando \textit{batches} de \textit{queries} deben parar su ejecución porque transacciones de escritura han llegado al sistema, y este deben actualizar la información del índice invertido. Solo después de la fase de actualización el sistema es capaz de ejecutar el siguiente \textit{batch} de transacciones de lectura. Al final de cada conjunto de consultas, es posible que algunos hilos de ejecución del sistema finalicen su trabajo y que no tengan más \textit{queries} para procesar, por lo que ellos tienen que esperar que los \textit{threads} restantes finalicen su trabajo antes que el sistema entre en la fase de actualización de su índice invertido o bien, se pase a la ejecución del siguiente \textit{batch} de consultas.
Sin embargo, aunque cada hilo de ejecución está secuencialmente ejecutando una transacción de lectura diferente, algunas de estas operaciones puede tomar un tiempo cosiderable, de esta forma se produce una importante pérdida de eficiencia, aunque la intuición nos dice que esto se puede mitigar con \textit{queries} que requieran poca cantidad de tiempo para ser procesada (trabajos pequeños o \textit{small jobs}). 
En la Figura \ref{fig:small_jobs} queda reflejado lo dicho en el párrafo anterior. Si los trabajos que cada \textit{thread} está ejecutando son pequeños, entonces probablemente la pérdida de trabajo al final de cada \textit{batch} de consultas será menor al trabajo que se pierde cuando los trabajos son grandes (ver Figura \ref{fig:large_jobs}).  


\begin{figure}[H]
\centering
\includegraphics[scale=.75]{images/small_jobs.eps}
\caption{Ejecución en paralelo de \textit{small jobs}}
\label{fig:small_jobs}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=.75]{images/large_jobs.eps}
\caption{Ejecución en paralelo de \textit{large jobs}}
\label{fig:large_jobs}
\end{figure}